<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>CS6620 Cloud Computing Project Generator</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            min-height: 100vh;
            padding: 20px;
        }

        .container {
            background: white;
            border-radius: 16px;
            box-shadow: 0 10px 40px rgba(0, 0, 0, 0.2);
            max-width: 1200px;
            margin: 0 auto;
            padding: 40px;
        }

        h1 {
            text-align: center;
            color: #1e3c72;
            margin-bottom: 10px;
            font-size: 2rem;
        }

        .subtitle {
            text-align: center;
            color: #666;
            margin-bottom: 30px;
        }

        .info-box {
            background: #E3F2FD;
            border-left: 4px solid #1e3c72;
            padding: 15px;
            margin-bottom: 30px;
            border-radius: 4px;
        }

        .info-box p {
            margin: 0;
            color: #1565C0;
        }

        .filters {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
            background: #f5f5f5;
            padding: 25px;
            border-radius: 12px;
        }

        .filter-group {
            display: flex;
            flex-direction: column;
        }

        label {
            font-weight: 600;
            color: #1e3c72;
            margin-bottom: 8px;
            font-size: 0.9rem;
        }

        select {
            padding: 10px;
            border: 2px solid #ddd;
            border-radius: 6px;
            font-size: 15px;
            background: white;
            cursor: pointer;
            transition: border-color 0.3s;
        }

        select:hover, select:focus {
            border-color: #2a5298;
            outline: none;
        }

        .generate-btn {
            width: 100%;
            padding: 14px;
            background: linear-gradient(135deg, #1e3c72, #2a5298);
            color: white;
            border: none;
            border-radius: 8px;
            font-size: 16px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s ease;
            margin-bottom: 30px;
        }

        .generate-btn:hover {
            transform: translateY(-2px);
            box-shadow: 0 6px 20px rgba(30, 60, 114, 0.3);
        }

        .project-card {
            background: #fff;
            border: 1px solid #e0e0e0;
            border-radius: 12px;
            padding: 35px;
            box-shadow: 0 2px 8px rgba(0, 0, 0, 0.05);
        }

        .project-header {
            border-bottom: 2px solid #f0f0f0;
            padding-bottom: 20px;
            margin-bottom: 25px;
        }

        .project-title {
            font-size: 1.5rem;
            color: #1e3c72;
            margin-bottom: 10px;
        }

        .project-description {
            color: #555;
            line-height: 1.6;
        }

        .complexity-badge {
            display: inline-block;
            padding: 4px 12px;
            border-radius: 12px;
            font-size: 0.85rem;
            font-weight: 600;
            margin-top: 10px;
        }

        .intermediate {
            background: #FFF3E0;
            color: #E65100;
        }

        .advanced {
            background: #FFEBEE;
            color: #C62828;
        }

        .section {
            margin-bottom: 25px;
        }

        .section-title {
            font-weight: 600;
            color: #1e3c72;
            font-size: 1.1rem;
            margin-bottom: 12px;
            display: flex;
            align-items: center;
        }

        .section-title::before {
            content: "▶";
            margin-right: 8px;
            font-size: 0.8rem;
        }

        .architecture-box {
            background: #f8f8f8;
            border: 2px dashed #ccc;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 15px;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            white-space: pre-wrap;
            line-height: 1.5;
            overflow-x: auto;
        }

        .tech-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 15px;
        }

        .tech-column {
            background: #f5f5f5;
            padding: 20px;
            border-radius: 8px;
        }

        .tech-column h4 {
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            gap: 8px;
        }

        .aws-column h4 {
            color: #FF9900;
        }

        .oss-column h4 {
            color: #4CAF50;
        }

        .tech-list {
            list-style: none;
            padding: 0;
        }

        .tech-list li {
            padding: 8px 0;
            border-bottom: 1px solid #e0e0e0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }

        .tech-list li:last-child {
            border-bottom: none;
        }

        .cost-indicator {
            font-size: 0.85rem;
            color: #666;
        }

        .dataset-info {
            background: #E6F7FF;
            border: 1px solid #1890FF;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 15px;
        }

        .dataset-link {
            color: #1890FF;
            text-decoration: none;
            font-weight: 500;
        }

        .dataset-link:hover {
            text-decoration: underline;
        }

        .implementation-tabs {
            display: flex;
            gap: 10px;
            margin-bottom: 15px;
            border-bottom: 2px solid #e0e0e0;
        }

        .tab {
            padding: 10px 20px;
            cursor: pointer;
            border-radius: 8px 8px 0 0;
            font-weight: 600;
            transition: all 0.3s;
        }

        .tab.active {
            background: #f5f5f5;
            border-bottom: 2px solid #1e3c72;
            margin-bottom: -2px;
        }

        .tab-content {
            display: none;
        }

        .tab-content.active {
            display: block;
        }

        .implementation-list {
            list-style: none;
            padding-left: 0;
        }

        .implementation-list li {
            position: relative;
            padding-left: 30px;
            margin-bottom: 12px;
            line-height: 1.6;
        }

        .implementation-list li::before {
            content: "✓";
            position: absolute;
            left: 0;
            color: #4CAF50;
            font-weight: bold;
        }

        .code-snippet {
            background: #2D2D2D;
            color: #F8F8F2;
            padding: 15px;
            border-radius: 6px;
            margin: 10px 0;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 0.85rem;
            overflow-x: auto;
        }

        .deliverables-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 15px;
            margin-top: 10px;
        }

        .deliverable-item {
            background: #F5F5F5;
            padding: 12px;
            border-radius: 6px;
            border-left: 3px solid #1e3c72;
        }

        .time-estimate {
            background: #F0F0F0;
            border-radius: 6px;
            padding: 10px;
            margin-top: 15px;
            text-align: center;
            font-weight: 500;
        }

        .learner-lab-note {
            background: #FFF4E6;
            border: 1px solid #FFD591;
            border-radius: 8px;
            padding: 15px;
            margin-top: 15px;
        }

        .learner-lab-title {
            font-weight: 600;
            color: #D46B08;
            margin-bottom: 8px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Cloud Computing Project Generator</h1>
        <p class="subtitle">CS6620 - Fall 2025 | Real-World Projects with AWS & Open Source Tools</p>
        
        <div class="info-box">
            <p>📚 Projects combine AWS services with open-source alternatives, using real datasets for practical learning.</p>
        </div>

        <div class="filters">
            <div class="filter-group">
                <label for="projectFocus">Project Focus Area</label>
                <select id="projectFocus">
                    <option value="web">Web Applications & CDN</option>
                    <option value="containers">Containers & Docker</option>
                    <option value="data">Data Processing & Analytics</option>
                    <option value="monitoring">Monitoring & Logging</option>
                    <option value="security">Security & Compliance</option>
                    <option value="automation">CI/CD & Automation</option>
                    <option value="serverless">Serverless Architecture</option>
                    <option value="infrastructure">Infrastructure as Code</option>
                </select>
            </div>

            <div class="filter-group">
                <label for="complexity">Complexity Level</label>
                <select id="complexity">
                    <option value="intermediate">Intermediate (3-4 hours)</option>
                    <option value="advanced">Advanced (6-8 hours)</option>
                </select>
            </div>

            <div class="filter-group">
                <label for="dataSize">Dataset Size</label>
                <select id="dataSize">
                    <option value="any">Any Size</option>
                    <option value="small">Small (<100MB)</option>
                    <option value="medium">Medium (100MB-1GB)</option>
                    <option value="streaming">Real-time/Streaming</option>
                </select>
            </div>
        </div>

        <button class="generate-btn" onclick="generateProject()">Generate Project</button>

        <div id="projectOutput"></div>
    </div>

    <script>
        const projects = {
            web: {
                intermediate: [
                    {
                        title: "E-Commerce Website with Multi-CDN Strategy",
                        description: "Deploy a product catalog website using S3/CloudFront and compare with Nginx/Cloudflare CDN for performance and cost optimization.",
                        complexity: "intermediate",
                        architecture: `User Request → Route 53 (DNS)
         ↓
    Primary CDN
    ├── CloudFront → S3 Static Assets
    └── Cloudflare → EC2 Nginx Server
         ↓
    Application Layer (EC2)
    ├── Node.js/Python App
    └── Product API
         ↓
    Data Layer
    ├── RDS MySQL (Products)
    └── ElastiCache (Sessions)`,
                        awsTools: {
                            "S3": "Static asset storage",
                            "CloudFront": "CDN distribution",
                            "EC2": "Web servers",
                            "RDS": "Product database",
                            "ElastiCache": "Session cache"
                        },
                        ossTools: {
                            "Nginx": "Web server/reverse proxy",
                            "Cloudflare": "CDN (free tier)",
                            "MySQL": "Database",
                            "Redis": "Caching",
                            "PM2": "Process manager"
                        },
                        dataset: {
                            name: "Amazon Product Dataset",
                            size: "50MB",
                            description: "5000 products with images, descriptions, and reviews",
                            link: "https://www.kaggle.com/datasets/promptcloud/amazon-product-dataset-2020",
                            format: "CSV with product data, image URLs"
                        },
                        implementationAWS: [
                            "Create S3 bucket for static assets",
                            "Set up CloudFront distribution",
                            "Launch EC2 instances for app servers",
                            "Configure RDS MySQL instance",
                            "Set up ElastiCache Redis cluster",
                            "Implement auto-scaling group",
                            "Configure Route 53 for DNS",
                            "Set up CloudWatch monitoring"
                        ],
                        implementationOSS: [
                            "Install Nginx on EC2",
                            "Configure Cloudflare CDN",
                            "Set up PM2 for Node.js",
                            "Install MySQL locally",
                            "Configure Redis caching",
                            "Set up Nginx caching rules",
                            "Implement log rotation",
                            "Create backup scripts"
                        ],
                        learnerLabNotes: "Use t2.micro for web servers. RDS free tier includes 750 hours of db.t2.micro.",
                        deliverables: [
                            "Working e-commerce site",
                            "Performance comparison (CloudFront vs Cloudflare)",
                            "Cost analysis spreadsheet",
                            "Architecture diagram",
                            "Load testing results",
                            "Caching strategy document"
                        ],
                        timeEstimate: "3-4 hours"
                    },
                    {
                        title: "WordPress High-Availability Setup",
                        description: "Create a scalable WordPress deployment comparing AWS managed services with open-source alternatives on EC2.",
                        complexity: "intermediate",
                        architecture: `Load Balancer (ALB/HAProxy)
         ↓
    Web Tier (Auto Scaling)
    ├── EC2 with WordPress
    └── Shared File System
         ↓
    Database Tier
    ├── RDS MySQL (AWS)
    OR
    └── MySQL with Replication (OSS)
         ↓
    Storage & Cache
    ├── EFS/S3 (Media)
    └── ElastiCache/Redis`,
                        awsTools: {
                            "ALB": "Load balancing",
                            "EC2 Auto Scaling": "Web servers",
                            "RDS": "Managed MySQL",
                            "EFS": "Shared storage",
                            "ElastiCache": "Object cache"
                        },
                        ossTools: {
                            "HAProxy": "Load balancer",
                            "Apache/Nginx": "Web server",
                            "MySQL": "Database",
                            "GlusterFS": "Distributed storage",
                            "Redis": "Object cache"
                        },
                        dataset: {
                            name: "WordPress Theme Unit Test",
                            size: "25MB",
                            description: "Complete WordPress test content including posts, pages, media",
                            link: "https://github.com/WordPress/theme-unit-test",
                            format: "WordPress XML export, sample images"
                        },
                        implementationAWS: [
                            "Set up VPC with public/private subnets",
                            "Create RDS MySQL instance",
                            "Launch EC2 instances in Auto Scaling group",
                            "Configure ALB with health checks",
                            "Set up EFS for shared uploads",
                            "Configure ElastiCache for sessions",
                            "Implement backup strategy",
                            "Set up CloudWatch alarms"
                        ],
                        implementationOSS: [
                            "Install HAProxy on dedicated EC2",
                            "Set up MySQL with master-slave replication",
                            "Configure GlusterFS across instances",
                            "Install Redis for object caching",
                            "Set up WordPress multisite",
                            "Implement backup scripts",
                            "Configure monitoring with Prometheus",
                            "Create failover procedures"
                        ],
                        learnerLabNotes: "EFS might not be available in Learner Labs. Use S3 with plugins as alternative.",
                        deliverables: [
                            "High-availability WordPress site",
                            "Performance benchmarks",
                            "Disaster recovery plan",
                            "Cost comparison (AWS vs OSS)",
                            "Scaling test results",
                            "Monitoring dashboards"
                        ],
                        timeEstimate: "4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "Multi-Region E-Learning Platform",
                        description: "Build a video-based learning platform with content delivery across multiple regions, comparing AWS and open-source solutions.",
                        complexity: "advanced",
                        architecture: `Global Load Balancer
    ├── US Region
    ├── EU Region (Simulated)
    └── APAC Region (Simulated)
         ↓
    Each Region:
    ├── Web Servers (ECS/Docker)
    ├── API Gateway/Kong
    ├── Video Processing
    │   ├── Lambda/FFmpeg
    │   └── S3/MinIO Storage
    └── Database
        ├── DynamoDB Global Tables
        OR
        └── CockroachDB`,
                        awsTools: {
                            "Route 53": "Global DNS",
                            "CloudFront": "Global CDN",
                            "S3": "Video storage",
                            "Lambda": "Video processing",
                            "DynamoDB": "Global database",
                            "Cognito": "User authentication"
                        },
                        ossTools: {
                            "Kong": "API Gateway",
                            "MinIO": "S3-compatible storage",
                            "FFmpeg": "Video processing",
                            "CockroachDB": "Distributed SQL",
                            "Keycloak": "Authentication",
                            "Docker": "Containerization"
                        },
                        dataset: {
                            name: "Open University Learning Analytics",
                            size: "500MB",
                            description: "Student interactions, course data, and sample educational videos",
                            link: "https://analyse.kmi.open.ac.uk/open_dataset",
                            format: "CSV files, MP4 sample videos"
                        },
                        implementationAWS: [
                            "Design multi-region architecture",
                            "Set up S3 buckets with replication",
                            "Configure CloudFront distributions",
                            "Create Lambda functions for video processing",
                            "Set up DynamoDB global tables",
                            "Implement Cognito user pools",
                            "Configure API Gateway",
                            "Build cost optimization strategies"
                        ],
                        implementationOSS: [
                            "Deploy MinIO clusters",
                            "Set up Kong API Gateway",
                            "Configure CockroachDB geo-replication",
                            "Implement FFmpeg processing pipeline",
                            "Deploy Keycloak for auth",
                            "Set up Docker Swarm/K8s",
                            "Implement CDN with Varnish",
                            "Create monitoring stack"
                        ],
                        learnerLabNotes: "Simulate multi-region with different VPCs. Use smaller video files for testing.",
                        deliverables: [
                            "Working learning platform",
                            "Multi-region deployment guide",
                            "Performance analysis across regions",
                            "Cost optimization report",
                            "Video processing pipeline",
                            "User authentication flow",
                            "Disaster recovery procedures"
                        ],
                        timeEstimate: "6-8 hours"
                    }
                ]
            },
            containers: {
                intermediate: [
                    {
                        title: "Microservices with Docker Compose vs ECS",
                        description: "Deploy a microservices application using Docker Compose and AWS ECS, comparing deployment complexity and costs.",
                        complexity: "intermediate",
                        architecture: `API Gateway
         ↓
    Microservices
    ├── User Service
    ├── Product Service
    ├── Order Service
    └── Notification Service
         ↓
    Data Stores
    ├── PostgreSQL (Users)
    ├── MongoDB (Products)
    ├── Redis (Cache)
    └── RabbitMQ (Messages)`,
                        awsTools: {
                            "ECS": "Container orchestration",
                            "ECR": "Container registry",
                            "ALB": "Load balancing",
                            "RDS": "PostgreSQL",
                            "DocumentDB": "MongoDB-compatible"
                        },
                        ossTools: {
                            "Docker Compose": "Local orchestration",
                            "Traefik": "Reverse proxy",
                            "PostgreSQL": "User database",
                            "MongoDB": "Product database",
                            "RabbitMQ": "Message queue"
                        },
                        dataset: {
                            name: "E-Commerce Microservices Dataset",
                            size: "100MB",
                            description: "Users, products, orders, and transaction data",
                            link: "https://github.com/GoogleCloudPlatform/microservices-demo",
                            format: "JSON/CSV data files"
                        },
                        implementationAWS: [
                            "Create ECS cluster",
                            "Build and push images to ECR",
                            "Define task definitions",
                            "Create ECS services",
                            "Configure ALB target groups",
                            "Set up RDS and DocumentDB",
                            "Implement service discovery",
                            "Configure CloudWatch logs"
                        ],
                        implementationOSS: [
                            "Write docker-compose.yml",
                            "Create Dockerfiles for each service",
                            "Set up Traefik routing",
                            "Configure service networking",
                            "Implement health checks",
                            "Set up persistent volumes",
                            "Configure RabbitMQ clustering",
                            "Create backup strategies"
                        ],
                        learnerLabNotes: "ECS with Fargate might have limitations. Use EC2 launch type for full control.",
                        deliverables: [
                            "Working microservices application",
                            "Deployment comparison guide",
                            "Service communication diagram",
                            "Performance metrics",
                            "Cost analysis",
                            "Troubleshooting guide"
                        ],
                        timeEstimate: "4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "Container Security Scanner Pipeline",
                        description: "Build an automated container security scanning pipeline comparing AWS native tools with open-source alternatives.",
                        complexity: "advanced",
                        architecture: `Git Push → CI/CD Pipeline
         ↓
    Build Stage
    ├── Docker Build
    └── Unit Tests
         ↓
    Security Scanning
    ├── Image Scanning
    │   ├── ECR Scanning
    │   └── Trivy/Clair
    ├── Secrets Detection
    └── License Compliance
         ↓
    Registry (ECR/Harbor)
         ↓
    Deployment (Only if secure)`,
                        awsTools: {
                            "CodePipeline": "CI/CD orchestration",
                            "CodeBuild": "Build environment",
                            "ECR": "Container registry with scanning",
                            "Security Hub": "Findings aggregation",
                            "Lambda": "Custom validations"
                        },
                        ossTools: {
                            "Jenkins": "CI/CD server",
                            "Trivy": "Vulnerability scanner",
                            "GitLab": "Source control",
                            "Harbor": "Secure registry",
                            "SonarQube": "Code quality"
                        },
                        dataset: {
                            name: "Vulnerable Container Images",
                            size: "200MB",
                            description: "Deliberately vulnerable containers for testing",
                            link: "https://github.com/vulnerables/web-dvwa",
                            format: "Dockerfiles, vulnerable applications"
                        },
                        implementationAWS: [
                            "Set up CodePipeline with stages",
                            "Configure CodeBuild for Docker",
                            "Enable ECR vulnerability scanning",
                            "Create Lambda for custom checks",
                            "Configure Security Hub integration",
                            "Set up SNS notifications",
                            "Implement break-the-build logic",
                            "Create compliance reports"
                        ],
                        implementationOSS: [
                            "Install Jenkins with plugins",
                            "Set up Trivy scanning",
                            "Configure GitLab CI/CD",
                            "Deploy Harbor with Clair",
                            "Integrate SonarQube analysis",
                            "Create scanning policies",
                            "Build reporting dashboard",
                            "Implement remediation workflow"
                        ],
                        learnerLabNotes: "Focus on scanning and reporting rather than fixing all vulnerabilities.",
                        deliverables: [
                            "Automated scanning pipeline",
                            "Security findings report",
                            "Remediation guidelines",
                            "Pipeline performance metrics",
                            "Cost comparison",
                            "Best practices document"
                        ],
                        timeEstimate: "6 hours"
                    }
                ]
            },
            data: {
                intermediate: [
                    {
                        title: "Real-Time Analytics: Kinesis vs Apache Kafka",
                        description: "Build a real-time analytics pipeline for IoT sensor data comparing AWS Kinesis with Apache Kafka.",
                        complexity: "intermediate",
                        architecture: `IoT Sensors (Simulated)
         ↓
    Data Ingestion
    ├── Kinesis Data Streams
    OR
    └── Kafka Cluster
         ↓
    Stream Processing
    ├── Kinesis Analytics
    OR
    └── Kafka Streams
         ↓
    Storage & Analytics
    ├── S3 Data Lake
    ├── Elasticsearch
    └── Dashboard (QuickSight/Grafana)`,
                        awsTools: {
                            "Kinesis Data Streams": "Data ingestion",
                            "Kinesis Analytics": "Stream processing",
                            "Kinesis Firehose": "Data delivery",
                            "S3": "Data lake storage",
                            "Athena": "SQL analytics"
                        },
                        ossTools: {
                            "Apache Kafka": "Message broker",
                            "Kafka Streams": "Processing",
                            "Elasticsearch": "Search/Analytics",
                            "Logstash": "Data pipeline",
                            "Grafana": "Visualization"
                        },
                        dataset: {
                            name: "IoT Sensor Stream Generator",
                            size: "Streaming",
                            description: "Python script generating temperature, humidity, pressure data",
                            link: "https://github.com/aws-samples/iot-device-simulator",
                            format: "JSON sensor readings"
                        },
                        implementationAWS: [
                            "Create Kinesis data stream",
                            "Set up data generator Lambda",
                            "Configure Kinesis Analytics app",
                            "Set up Firehose to S3",
                            "Create Glue crawler",
                            "Build Athena queries",
                            "Create QuickSight dashboard",
                            "Set up CloudWatch alarms"
                        ],
                        implementationOSS: [
                            "Deploy Kafka cluster (or MSK)",
                            "Create Kafka topics",
                            "Build producer application",
                            "Implement Kafka Streams app",
                            "Set up Elasticsearch",
                            "Configure Logstash pipelines",
                            "Create Grafana dashboards",
                            "Implement data retention"
                        ],
                        learnerLabNotes: "Start with single Kafka broker for simplicity. Use t2.small for Kafka.",
                        deliverables: [
                            "Working streaming pipeline",
                            "Real-time dashboard",
                            "Performance comparison",
                            "Cost analysis at different scales",
                            "Data retention strategy",
                            "Monitoring and alerts"
                        ],
                        timeEstimate: "4 hours",
                        sampleCode: `# Kinesis Data Generator
import json
import boto3
import random
from datetime import datetime

kinesis = boto3.client('kinesis')

def generate_sensor_data():
    return {
        'sensor_id': f'sensor_{random.randint(1,100)}',
        'temperature': round(random.uniform(15, 35), 2),
        'humidity': round(random.uniform(30, 80), 2),
        'timestamp': datetime.now().isoformat()
    }

# Send to Kinesis
data = generate_sensor_data()
kinesis.put_record(
    StreamName='iot-sensor-stream',
    Data=json.dumps(data),
    PartitionKey=data['sensor_id']
)`
                    }
                ],
                advanced: [
                    {
                        title: "Data Lake Platform with CDC and Analytics",
                        description: "Build a modern data lake with change data capture (CDC) from databases, comparing AWS native with open-source tools.",
                        complexity: "advanced",
                        architecture: `Source Databases
    ├── RDS MySQL (Transactional)
    └── DynamoDB (NoSQL)
         ↓
    CDC Pipeline
    ├── DMS (AWS)
    OR
    └── Debezium (OSS)
         ↓
    Stream Processing
    ├── Kinesis + Lambda
    OR
    └── Kafka + Flink
         ↓
    Data Lake (S3)
    ├── Raw Zone
    ├── Processed Zone
    └── Curated Zone
         ↓
    Analytics
    ├── Athena + QuickSight
    └── Presto + Superset`,
                        awsTools: {
                            "DMS": "Database migration/CDC",
                            "Kinesis": "Streaming",
                            "Glue": "ETL and catalog",
                            "S3": "Data lake storage",
                            "Athena": "SQL queries",
                            "QuickSight": "BI dashboards"
                        },
                        ossTools: {
                            "Debezium": "CDC platform",
                            "Apache Kafka": "Streaming",
                            "Apache Flink": "Stream processing",
                            "Apache Hudi": "Data lake format",
                            "Presto": "SQL engine",
                            "Superset": "BI tool"
                        },
                        dataset: {
                            name: "TPC-DS Benchmark Dataset",
                            size: "1GB",
                            description: "Retail sales data with fact and dimension tables",
                            link: "http://www.tpc.org/tpc_documents_current_versions/pdf/tpc-ds_v3.2.0.pdf",
                            format: "CSV files, SQL scripts"
                        },
                        implementationAWS: [
                            "Set up source RDS database",
                            "Configure DMS for CDC",
                            "Create Kinesis streams",
                            "Build Lambda processors",
                            "Design S3 bucket structure",
                            "Set up Glue crawlers",
                            "Create Athena tables",
                            "Build QuickSight dashboards"
                        ],
                        implementationOSS: [
                            "Deploy Debezium connectors",
                            "Configure Kafka Connect",
                            "Set up Flink jobs",
                            "Implement Hudi tables",
                            "Deploy Presto cluster",
                            "Configure Superset",
                            "Build data quality checks",
                            "Create lineage tracking"
                        ],
                        learnerLabNotes: "DMS requires specific instance types. Start with small data volumes.",
                        deliverables: [
                            "Complete data lake platform",
                            "CDC pipeline documentation",
                            "Data quality reports",
                            "Performance benchmarks",
                            "Cost analysis",
                            "BI dashboard examples",
                            "Data governance policies"
                        ],
                        timeEstimate: "8 hours"
                    }
                ]
            },
            monitoring: {
                intermediate: [
                    {
                        title: "Full-Stack Observability Platform",
                        description: "Implement comprehensive monitoring for a web application using CloudWatch and Prometheus/Grafana stack.",
                        complexity: "intermediate",
                        architecture: `Web Application (ECS/EC2)
    ├── Frontend (React)
    ├── Backend API
    └── Database
         ↓
    Metrics Collection
    ├── CloudWatch Agent
    └── Prometheus Exporters
         ↓
    Log Aggregation
    ├── CloudWatch Logs
    └── Loki
         ↓
    Visualization
    ├── CloudWatch Dashboard
    └── Grafana`,
                        awsTools: {
                            "CloudWatch": "Metrics and logs",
                            "X-Ray": "Distributed tracing",
                            "SNS": "Alerting",
                            "Systems Manager": "Inventory",
                            "CloudWatch Insights": "Log analysis"
                        },
                        ossTools: {
                            "Prometheus": "Metrics collection",
                            "Grafana": "Dashboards",
                            "Loki": "Log aggregation",
                            "AlertManager": "Alert routing",
                            "Jaeger": "Distributed tracing"
                        },
                        dataset: {
                            name: "Sample Application Logs",
                            size: "100MB",
                            description: "Web server logs, application logs, and performance metrics",
                            link: "https://github.com/elastic/examples/tree/master/Common%20Data%20Formats",
                            format: "Log files, metric samples"
                        },
                        implementationAWS: [
                            "Install CloudWatch agent",
                            "Configure custom metrics",
                            "Set up log groups",
                            "Create CloudWatch dashboards",
                            "Configure X-Ray tracing",
                            "Set up SNS topics",
                            "Create CloudWatch alarms",
                            "Build Insights queries"
                        ],
                        implementationOSS: [
                            "Deploy Prometheus server",
                            "Configure exporters",
                            "Set up Grafana",
                            "Install Loki for logs",
                            "Configure AlertManager",
                            "Create Grafana dashboards",
                            "Set up Jaeger tracing",
                            "Implement SLO monitoring"
                        ],
                        learnerLabNotes: "Prometheus requires persistent storage. Use EBS volumes for data retention.",
                        deliverables: [
                            "Complete monitoring setup",
                            "Dashboard screenshots",
                            "Alert configuration",
                            "Performance baselines",
                            "Cost comparison",
                            "Runbook documentation"
                        ],
                        timeEstimate: "3-4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "AI-Powered Anomaly Detection System",
                        description: "Build an intelligent monitoring system that detects anomalies using ML, comparing AWS and open-source approaches.",
                        complexity: "advanced",
                        architecture: `Data Sources
    ├── Application Metrics
    ├── Infrastructure Logs
    └── Business KPIs
         ↓
    Data Pipeline
    ├── Kinesis/Kafka
    └── S3 Data Lake
         ↓
    ML Processing
    ├── SageMaker (AWS)
    OR
    └── Prophet + MLflow (OSS)
         ↓
    Anomaly Detection
    ├── Real-time Alerts
    ├── Root Cause Analysis
    └── Predictive Insights
         ↓
    Visualization
    └── Custom Dashboards`,
                        awsTools: {
                            "SageMaker": "ML platform",
                            "Kinesis": "Data streaming",
                            "CloudWatch": "Metrics source",
                            "Lambda": "Processing",
                            "QuickSight": "ML insights"
                        },
                        ossTools: {
                            "Prophet": "Time series forecasting",
                            "MLflow": "ML lifecycle",
                            "Kafka": "Streaming",
                            "InfluxDB": "Time series DB",
                            "Grafana": "Visualization"
                        },
                        dataset: {
                            name: "NASA Anomaly Detection Dataset",
                            size: "500MB",
                            description: "Time series data with labeled anomalies",
                            link: "https://github.com/numenta/NAB",
                            format: "CSV time series data"
                        },
                        implementationAWS: [
                            "Set up data pipeline to S3",
                            "Create SageMaker notebooks",
                            "Train anomaly detection models",
                            "Deploy model endpoints",
                            "Build Lambda for inference",
                            "Create automated responses",
                            "Set up notification system",
                            "Build ML insights dashboard"
                        ],
                        implementationOSS: [
                            "Deploy time series database",
                            "Set up Prophet models",
                            "Configure MLflow tracking",
                            "Build anomaly detection pipeline",
                            "Create alert mechanisms",
                            "Implement root cause analysis",
                            "Build custom dashboards",
                            "Create feedback loop"
                        ],
                        learnerLabNotes: "SageMaker training can be expensive. Use small datasets and stop instances when not needed.",
                        deliverables: [
                            "Working anomaly detection system",
                            "ML model documentation",
                            "Detection accuracy report",
                            "False positive analysis",
                            "Cost optimization guide",
                            "Operational playbook",
                            "Architecture decisions"
                        ],
                        timeEstimate: "6-8 hours"
                    }
                ]
            },
            security: {
                intermediate: [
                    {
                        title: "Zero-Trust Network Architecture",
                        description: "Implement a zero-trust security model for a web application using AWS and open-source tools.",
                        complexity: "intermediate",
                        architecture: `Internet → WAF/CloudFlare
         ↓
    Identity Verification
    ├── Cognito/Auth0
    └── MFA Required
         ↓
    API Gateway with mTLS
         ↓
    Micro-segmented Network
    ├── App Tier (Private)
    ├── Database (Isolated)
    └── Admin (Restricted)
         ↓
    Continuous Verification
    └── Session + Device Trust`,
                        awsTools: {
                            "WAF": "Web application firewall",
                            "Cognito": "User authentication",
                            "API Gateway": "API security",
                            "VPC": "Network isolation",
                            "Secrets Manager": "Credential storage"
                        },
                        ossTools: {
                            "Keycloak": "Identity management",
                            "Vault": "Secrets management",
                            "Open Policy Agent": "Policy engine",
                            "WireGuard": "VPN",
                            "fail2ban": "Intrusion prevention"
                        },
                        dataset: {
                            name: "Security Event Logs",
                            size: "200MB",
                            description: "Sample authentication logs, attack patterns, and user behavior data",
                            link: "https://www.secrepo.com/",
                            format: "Log files, JSON events"
                        },
                        implementationAWS: [
                            "Design VPC with micro-segmentation",
                            "Configure WAF rules",
                            "Set up Cognito with MFA",
                            "Implement API Gateway auth",
                            "Configure security groups",
                            "Enable VPC Flow Logs",
                            "Set up CloudTrail",
                            "Create incident response plan"
                        ],
                        implementationOSS: [
                            "Deploy Keycloak for SSO",
                            "Configure Vault for secrets",
                            "Implement OPA policies",
                            "Set up WireGuard VPN",
                            "Configure fail2ban",
                            "Implement certificate management",
                            "Create security monitoring",
                            "Build compliance reports"
                        ],
                        learnerLabNotes: "Focus on network segmentation and authentication. WAF rules can be basic.",
                        deliverables: [
                            "Zero-trust implementation",
                            "Security architecture diagram",
                            "Access control matrix",
                            "Incident response plan",
                            "Compliance checklist",
                            "Performance impact analysis"
                        ],
                        timeEstimate: "4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "Cloud Security Operations Center (SOC)",
                        description: "Build a complete security operations center using AWS services and open-source SIEM tools.",
                        complexity: "advanced",
                        architecture: `Security Data Sources
    ├── VPC Flow Logs
    ├── CloudTrail
    ├── GuardDuty
    ├── Application Logs
    └── Third-party APIs
         ↓
    Collection & Normalization
    ├── Kinesis Firehose
    └── Logstash
         ↓
    SIEM Platform
    ├── Security Hub (AWS)
    └── Elastic Security (OSS)
         ↓
    Analysis & Response
    ├── Threat Intelligence
    ├── Automated Remediation
    └── Incident Management`,
                        awsTools: {
                            "Security Hub": "Finding aggregation",
                            "GuardDuty": "Threat detection",
                            "Detective": "Investigation",
                            "Config": "Compliance",
                            "Lambda": "Auto-remediation"
                        },
                        ossTools: {
                            "Elastic Security": "SIEM platform",
                            "TheHive": "Incident response",
                            "MISP": "Threat intelligence",
                            "Cortex": "Observable analysis",
                            "Wazuh": "Host security"
                        },
                        dataset: {
                            name: "BOTS Dataset v3",
                            size: "3GB",
                            description: "Comprehensive security dataset with various attack scenarios",
                            link: "https://github.com/splunk/botsv3",
                            format: "Various log formats"
                        },
                        implementationAWS: [
                            "Enable all security services",
                            "Configure Security Hub",
                            "Set up custom findings",
                            "Create Lambda remediation",
                            "Build Detective graphs",
                            "Configure Config rules",
                            "Create response runbooks",
                            "Implement cost controls"
                        ],
                        implementationOSS: [
                            "Deploy Elastic Security",
                            "Configure TheHive",
                            "Set up MISP feeds",
                            "Integrate Cortex analyzers",
                            "Deploy Wazuh agents",
                            "Create detection rules",
                            "Build response playbooks",
                            "Implement metrics tracking"
                        ],
                        learnerLabNotes: "Use smaller data samples. Focus on detection and alerting rather than full incident response.",
                        deliverables: [
                            "Complete SOC platform",
                            "Detection rule library",
                            "Incident response playbooks",
                            "Threat intelligence integration",
                            "Security metrics dashboard",
                            "Cost optimization report",
                            "Training documentation"
                        ],
                        timeEstimate: "8 hours"
                    }
                ]
            },
            automation: {
                intermediate: [
                    {
                        title: "GitOps CI/CD Pipeline Comparison",
                        description: "Build CI/CD pipelines using AWS CodePipeline and GitLab CI, implementing GitOps principles.",
                        complexity: "intermediate",
                        architecture: `Git Repository
         ↓
    CI Pipeline
    ├── Build
    ├── Test
    ├── Security Scan
    └── Package
         ↓
    Artifact Storage
    ├── S3/ECR
    └── GitLab Registry
         ↓
    CD Pipeline
    ├── Dev Deploy
    ├── Stage Deploy
    └── Prod Deploy (Manual)
         ↓
    Monitoring & Rollback`,
                        awsTools: {
                            "CodeCommit": "Source control",
                            "CodeBuild": "Build service",
                            "CodePipeline": "Orchestration",
                            "CodeDeploy": "Deployment",
                            "CloudFormation": "IaC"
                        },
                        ossTools: {
                            "GitLab": "Source + CI/CD",
                            "Jenkins": "Build server",
                            "ArgoCD": "GitOps deployment",
                            "Terraform": "Infrastructure",
                            "Ansible": "Configuration"
                        },
                        dataset: {
                            name: "Sample Microservices Project",
                            size: "50MB",
                            description: "Multi-language microservices with tests and IaC",
                            link: "https://github.com/GoogleCloudPlatform/microservices-demo",
                            format: "Source code, Dockerfiles, K8s manifests"
                        },
                        implementationAWS: [
                            "Set up CodeCommit repo",
                            "Create CodeBuild projects",
                            "Configure CodePipeline",
                            "Set up CodeDeploy",
                            "Create CloudFormation templates",
                            "Configure approval gates",
                            "Set up notifications",
                            "Implement rollback strategy"
                        ],
                        implementationOSS: [
                            "Configure GitLab CI/CD",
                            "Set up Jenkins agents",
                            "Deploy ArgoCD",
                            "Create Terraform modules",
                            "Write Ansible playbooks",
                            "Implement GitOps workflow",
                            "Configure branch protection",
                            "Create deployment dashboards"
                        ],
                        learnerLabNotes: "CodePipeline has some limitations in Learner Labs. Focus on the comparison aspects.",
                        deliverables: [
                            "Working CI/CD pipelines",
                            "Pipeline comparison matrix",
                            "Deployment time metrics",
                            "Cost analysis",
                            "Security scanning reports",
                            "Best practices guide"
                        ],
                        timeEstimate: "4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "Self-Healing Infrastructure Platform",
                        description: "Build an automated infrastructure platform that detects and fixes issues without human intervention.",
                        complexity: "advanced",
                        architecture: `Infrastructure Components
    ├── EC2 Auto Scaling
    ├── ECS Services
    ├── RDS Databases
    └── Load Balancers
         ↓
    Monitoring Layer
    ├── CloudWatch Metrics
    ├── Custom Health Checks
    └── Log Analysis
         ↓
    Decision Engine
    ├── Rule-based Logic
    ├── ML Predictions
    └── Chaos Testing
         ↓
    Remediation Actions
    ├── Auto-scaling
    ├── Service Restart
    ├── Failover
    └── Configuration Fix
         ↓
    Verification & Reporting`,
                        awsTools: {
                            "Systems Manager": "Automation",
                            "Lambda": "Remediation logic",
                            "EventBridge": "Event routing",
                            "Auto Scaling": "Capacity management",
                            "CloudWatch": "Monitoring"
                        },
                        ossTools: {
                            "Ansible": "Automation",
                            "Consul": "Service discovery",
                            "Nomad": "Orchestration",
                            "Prometheus": "Monitoring",
                            "Chaos Monkey": "Failure injection"
                        },
                        dataset: {
                            name: "Failure Scenario Library",
                            size: "10MB",
                            description: "Common failure patterns and remediation scripts",
                            link: "Custom dataset provided",
                            format: "YAML scenarios, Python scripts"
                        },
                        implementationAWS: [
                            "Set up Auto Scaling groups",
                            "Create Systems Manager documents",
                            "Configure EventBridge rules",
                            "Build Lambda functions",
                            "Implement health checks",
                            "Create remediation workflows",
                            "Set up chaos testing",
                            "Build reporting dashboard"
                        ],
                        implementationOSS: [
                            "Deploy Consul cluster",
                            "Configure Nomad jobs",
                            "Create Ansible playbooks",
                            "Set up Prometheus alerts",
                            "Implement Chaos Monkey",
                            "Build remediation logic",
                            "Create feedback loops",
                            "Implement learning system"
                        ],
                        learnerLabNotes: "Start with simple remediation scenarios. Chaos testing should be limited to non-critical components.",
                        deliverables: [
                            "Self-healing platform",
                            "Remediation playbooks",
                            "Failure test results",
                            "Recovery time metrics",
                            "Cost impact analysis",
                            "Lessons learned document",
                            "Architecture evolution"
                        ],
                        timeEstimate: "6-8 hours"
                    }
                ]
            },
            serverless: {
                intermediate: [
                    {
                        title: "Event-Driven Image Processing Pipeline",
                        description: "Build a serverless image processing pipeline comparing AWS Lambda with open-source alternatives.",
                        complexity: "intermediate",
                        architecture: `Image Upload → S3 Bucket
         ↓
    Event Trigger
    ├── S3 Event → Lambda
    OR
    └── MinIO → OpenFaaS
         ↓
    Processing Pipeline
    ├── Resize Images
    ├── Generate Thumbnails
    ├── Extract Metadata
    └── Apply Filters
         ↓
    Storage & CDN
    └── Processed Images`,
                        awsTools: {
                            "Lambda": "Image processing",
                            "S3": "Object storage",
                            "API Gateway": "Upload API",
                            "DynamoDB": "Metadata storage",
                            "CloudFront": "CDN"
                        },
                        ossTools: {
                            "OpenFaaS": "Functions platform",
                            "MinIO": "Object storage",
                            "ImageMagick": "Processing",
                            "MongoDB": "Metadata",
                            "Nginx": "Caching"
                        },
                        dataset: {
                            name: "Unsplash Sample Dataset",
                            size: "500MB",
                            description: "High-quality images in various formats and sizes",
                            link: "https://unsplash.com/data",
                            format: "JPEG, PNG, RAW images"
                        },
                        implementationAWS: [
                            "Create S3 buckets",
                            "Write Lambda functions",
                            "Configure S3 event triggers",
                            "Set up API Gateway",
                            "Create DynamoDB tables",
                            "Configure CloudFront",
                            "Implement error handling",
                            "Set up monitoring"
                        ],
                        implementationOSS: [
                            "Deploy OpenFaaS",
                            "Create function templates",
                            "Set up MinIO buckets",
                            "Configure event connectors",
                            "Deploy MongoDB",
                            "Implement caching layer",
                            "Create processing queue",
                            "Build monitoring stack"
                        ],
                        learnerLabNotes: "Lambda has 15-minute timeout. Process images in batches for efficiency.",
                        deliverables: [
                            "Working image pipeline",
                            "Performance benchmarks",
                            "Cost comparison",
                            "Processing time analysis",
                            "Error handling guide",
                            "Scaling strategies"
                        ],
                        timeEstimate: "3-4 hours",
                        sampleCode: `# Lambda image processor
import boto3
from PIL import Image
import io

def lambda_handler(event, context):
    s3 = boto3.client('s3')
    
    # Get image from S3
    bucket = event['Records'][0]['s3']['bucket']['name']
    key = event['Records'][0]['s3']['object']['key']
    
    image_object = s3.get_object(Bucket=bucket, Key=key)
    image = Image.open(io.BytesIO(image_object['Body'].read()))
    
    # Create thumbnail
    thumbnail = image.copy()
    thumbnail.thumbnail((128, 128))
    
    # Save to S3
    buffer = io.BytesIO()
    thumbnail.save(buffer, format=image.format)
    buffer.seek(0)
    
    s3.put_object(
        Bucket=f'{bucket}-thumbnails',
        Key=key,
        Body=buffer.getvalue()
    )
    
    return {'statusCode': 200}`
                    }
                ],
                advanced: [
                    {
                        title: "Serverless Data API with GraphQL",
                        description: "Build a complete serverless GraphQL API with real-time subscriptions, comparing AWS and open-source approaches.",
                        complexity: "advanced",
                        architecture: `GraphQL Client
         ↓
    API Layer
    ├── AppSync (AWS)
    OR
    └── Apollo Server (OSS)
         ↓
    Resolvers
    ├── Lambda Functions
    ├── Direct DB Access
    └── External APIs
         ↓
    Data Sources
    ├── DynamoDB
    ├── Aurora Serverless
    └── ElasticSearch
         ↓
    Real-time Subscriptions
    └── WebSocket Connections`,
                        awsTools: {
                            "AppSync": "GraphQL service",
                            "Lambda": "Resolver functions",
                            "DynamoDB": "NoSQL database",
                            "Aurora Serverless": "Relational DB",
                            "Cognito": "Authentication"
                        },
                        ossTools: {
                            "Apollo Server": "GraphQL server",
                            "Prisma": "ORM",
                            "Redis": "Pub/Sub",
                            "Auth0": "Authentication",
                            "Hasura": "GraphQL engine"
                        },
                        dataset: {
                            name: "Social Media Dataset",
                            size: "200MB",
                            description: "Users, posts, comments, likes, and relationships",
                            link: "https://snap.stanford.edu/data/soc-sign-bitcoin-otc.html",
                            format: "JSON, CSV relational data"
                        },
                        implementationAWS: [
                            "Set up AppSync API",
                            "Define GraphQL schema",
                            "Create Lambda resolvers",
                            "Configure data sources",
                            "Implement subscriptions",
                            "Set up Cognito auth",
                            "Create caching strategy",
                            "Build monitoring dashboard"
                        ],
                        implementationOSS: [
                            "Deploy Apollo Server",
                            "Configure Prisma ORM",
                            "Set up Redis pub/sub",
                            "Implement resolvers",
                            "Configure Auth0",
                            "Set up Hasura",
                            "Create subscription logic",
                            "Build performance tests"
                        ],
                        learnerLabNotes: "AppSync can get expensive with high request volumes. Monitor usage carefully.",
                        deliverables: [
                            "Complete GraphQL API",
                            "Schema documentation",
                            "Performance test results",
                            "Real-time demo",
                            "Cost analysis",
                            "Security assessment",
                            "Client integration guide"
                        ],
                        timeEstimate: "6-8 hours"
                    }
                ]
            },
            infrastructure: {
                intermediate: [
                    {
                        title: "Multi-Environment Infrastructure with Terraform",
                        description: "Create reusable Terraform modules for deploying consistent environments across dev, staging, and production.",
                        complexity: "intermediate",
                        architecture: `Terraform Modules
    ├── Networking Module
    ├── Compute Module
    ├── Database Module
    └── Security Module
         ↓
    Environments
    ├── Development
    ├── Staging
    └── Production
         ↓
    State Management
    ├── S3 Backend
    └── DynamoDB Locking`,
                        awsTools: {
                            "S3": "Terraform state",
                            "DynamoDB": "State locking",
                            "CloudFormation": "Comparison",
                            "Systems Manager": "Parameters",
                            "CodeBuild": "Terraform CI/CD"
                        },
                        ossTools: {
                            "Terraform": "IaC tool",
                            "Terragrunt": "DRY wrapper",
                            "tflint": "Linter",
                            "tfsec": "Security scanner",
                            "Atlantis": "Pull request automation"
                        },
                        dataset: {
                            name: "Infrastructure Templates",
                            size: "5MB",
                            description: "Sample Terraform configurations for common patterns",
                            link: "https://github.com/terraform-aws-modules",
                            format: "HCL configuration files"
                        },
                        implementationAWS: [
                            "Set up S3 state backend",
                            "Configure DynamoDB locking",
                            "Create VPC module",
                            "Build EC2/ASG module",
                            "Create RDS module",
                            "Implement security module",
                            "Set up environment configs",
                            "Create deployment pipeline"
                        ],
                        implementationOSS: [
                            "Structure Terraform modules",
                            "Implement Terragrunt",
                            "Configure tflint rules",
                            "Set up tfsec scanning",
                            "Deploy Atlantis",
                            "Create module tests",
                            "Build documentation",
                            "Implement GitOps workflow"
                        ],
                        learnerLabNotes: "Be careful with terraform destroy. Always plan before apply.",
                        deliverables: [
                            "Terraform module library",
                            "Environment configurations",
                            "Module documentation",
                            "Security scan reports",
                            "Cost estimates",
                            "CI/CD pipeline"
                        ],
                        timeEstimate: "4 hours"
                    }
                ],
                advanced: [
                    {
                        title: "Disaster Recovery Automation Platform",
                        description: "Build an automated disaster recovery system that can failover entire infrastructures across regions.",
                        complexity: "advanced",
                        architecture: `Primary Region (us-east-1)
    ├── VPC & Networking
    ├── Application Stack
    ├── RDS with Read Replica
    └── S3 with Replication
         ↓
    DR Orchestration
    ├── Health Monitoring
    ├── Failover Decision
    └── Recovery Automation
         ↓
    Secondary Region (us-west-2)
    ├── Standby Infrastructure
    ├── Data Replication
    └── DNS Failover
         ↓
    Recovery Validation
    └── Automated Testing`,
                        awsTools: {
                            "Route 53": "DNS failover",
                            "CloudFormation StackSets": "Multi-region",
                            "RDS": "Cross-region replicas",
                            "S3": "Cross-region replication",
                            "Lambda": "Orchestration"
                        },
                        ossTools: {
                            "Terraform": "Multi-region IaC",
                            "Ansible": "Orchestration",
                            "Consul": "Service discovery",
                            "Packer": "Image building",
                            "Chaos Toolkit": "DR testing"
                        },
                        dataset: {
                            name: "DR Test Scenarios",
                            size: "10MB",
                            description: "Failure scenarios, recovery procedures, and test data",
                            link: "Custom DR testing framework",
                            format: "YAML scenarios, test scripts"
                        },
                        implementationAWS: [
                            "Design multi-region architecture",
                            "Set up primary infrastructure",
                            "Configure replication",
                            "Create failover Lambda",
                            "Set up Route 53 health checks",
                            "Implement StackSets",
                            "Build recovery automation",
                            "Create validation tests"
                        ],
                        implementationOSS: [
                            "Create Terraform workspaces",
                            "Build Ansible playbooks",
                            "Configure Consul clusters",
                            "Create Packer images",
                            "Implement Chaos tests",
                            "Build orchestration logic",
                            "Create recovery dashboards",
                            "Document RTO/RPO"
                        ],
                        learnerLabNotes: "Simulate multi-region with different VPCs. Focus on automation logic rather than actual failover.",
                        deliverables: [
                            "DR automation platform",
                            "Recovery runbooks",
                            "RTO/RPO analysis",
                            "Cost impact assessment",
                            "Test result reports",
                            "Architecture documentation",
                            "Lessons learned"
                        ],
                        timeEstimate: "8 hours"
                    }
                ]
            }
        };

        let activeTab = 'aws';

        function setActiveTab(tab) {
            activeTab = tab;
            document.querySelectorAll('.tab').forEach(t => {
                t.classList.remove('active');
            });
            document.querySelector(`.tab[onclick="setActiveTab('${tab}')"]`).classList.add('active');
            
            document.querySelectorAll('.tab-content').forEach(content => {
                content.classList.remove('active');
            });
            document.getElementById(`${tab}-steps`).classList.add('active');
        }

        function generateProject() {
            const focus = document.getElementById('projectFocus').value;
            const complexity = document.getElementById('complexity').value;
            const dataSize = document.getElementById('dataSize').value;
            
            // Get projects for selected focus and complexity
            let availableProjects = [];
            
            if (projects[focus] && projects[focus][complexity]) {
                availableProjects = projects[focus][complexity];
            }
            
            // Filter by data size if specified
            if (dataSize !== 'any' && availableProjects.length > 0) {
                const filtered = availableProjects.filter(project => {
                    const size = project.dataset.size.toLowerCase();
                    if (dataSize === 'small') return size.includes('mb') && parseInt(size) < 100;
                    if (dataSize === 'medium') return size.includes('mb') && parseInt(size) >= 100 || size.includes('gb');
                    if (dataSize === 'streaming') return size.includes('stream') || project.dataset.name.includes('Stream');
                    return true;
                });
                if (filtered.length > 0) availableProjects = filtered;
            }
            
            if (availableProjects.length === 0) {
                alert('No projects found matching your criteria. Try different filters.');
                return;
            }
            
            // Select random project
            const project = availableProjects[Math.floor(Math.random() * availableProjects.length)];
            displayProject(project);
        }

        function displayProject(project) {
            const output = document.getElementById('projectOutput');
            
            // Generate implementation tabs if both AWS and OSS implementations exist
            let implementationSection = '';
            if (project.implementationAWS && project.implementationOSS) {
                implementationSection = `
                    <div class="section">
                        <h3 class="section-title">Implementation Steps</h3>
                        <div class="implementation-tabs">
                            <div class="tab active" onclick="setActiveTab('aws')">AWS Implementation</div>
                            <div class="tab" onclick="setActiveTab('oss')">Open Source Implementation</div>
                        </div>
                        <div class="tab-content active" id="aws-steps">
                            <ul class="implementation-list">
                                ${project.implementationAWS.map(step => `<li>${step}</li>`).join('')}
                            </ul>
                        </div>
                        <div class="tab-content" id="oss-steps">
                            <ul class="implementation-list">
                                ${project.implementationOSS.map(step => `<li>${step}</li>`).join('')}
                            </ul>
                        </div>
                    </div>
                `;
            } else if (project.implementationAWS) {
                implementationSection = `
                    <div class="section">
                        <h3 class="section-title">Implementation Steps</h3>
                        <ul class="implementation-list">
                            ${project.implementationAWS.map(step => `<li>${step}</li>`).join('')}
                        </ul>
                    </div>
                `;
            }
            
            // Generate tech comparison if both tools exist
            let techComparison = '';
            if (project.awsTools && project.ossTools) {
                techComparison = `
                    <div class="section">
                        <h3 class="section-title">Technology Stack</h3>
                        <div class="tech-comparison">
                            <div class="tech-column aws-column">
                                <h4>🔸 AWS Services</h4>
                                <ul class="tech-list">
                                    ${Object.entries(project.awsTools).map(([tool, use]) => 
                                        `<li><strong>${tool}</strong><span class="cost-indicator">${use}</span></li>`
                                    ).join('')}
                                </ul>
                            </div>
                            <div class="tech-column oss-column">
                                <h4>🔹 Open Source Tools</h4>
                                <ul class="tech-list">
                                    ${Object.entries(project.ossTools).map(([tool, use]) => 
                                        `<li><strong>${tool}</strong><span class="cost-indicator">${use}</span></li>`
                                    ).join('')}
                                </ul>
                            </div>
                        </div>
                    </div>
                `;
            }
            
            // Generate sample code section if exists
            let sampleCodeSection = '';
            if (project.sampleCode) {
                sampleCodeSection = `
                    <div class="section">
                        <h3 class="section-title">Sample Code</h3>
                        <div class="code-snippet">${project.sampleCode}</div>
                    </div>
                `;
            }
            
            output.innerHTML = `
                <div class="project-card">
                    <div class="project-header">
                        <h2 class="project-title">${project.title}</h2>
                        <p class="project-description">${project.description}</p>
                        <span class="complexity-badge ${project.complexity}">${project.complexity.toUpperCase()}</span>
                        <div class="time-estimate">
                            ⏱️ Estimated Time: ${project.timeEstimate}
                        </div>
                    </div>
                    
                    <div class="section">
                        <h3 class="section-title">Architecture Overview</h3>
                        <div class="architecture-box">${project.architecture}</div>
                    </div>
                    
                    ${techComparison}
                    
                    <div class="section">
                        <h3 class="section-title">Dataset Requirements</h3>
                        <div class="dataset-info">
                            <strong>${project.dataset.name}</strong> (${project.dataset.size})<br>
                            <p style="margin: 10px 0;">${project.dataset.description}</p>
                            <strong>Format:</strong> ${project.dataset.format}<br>
                            <strong>Source:</strong> <a href="${project.dataset.link}" class="dataset-link" target="_blank">${project.dataset.link}</a>
                        </div>
                    </div>
                    
                    ${implementationSection}
                    
                    ${sampleCodeSection}
                    
                    <div class="section">
                        <h3 class="section-title">AWS Learner Lab Notes</h3>
                        <div class="learner-lab-note">
                            <div class="learner-lab-title">⚠️ Important Constraints</div>
                            ${project.learnerLabNotes}
                        </div>
                    </div>
                    
                    <div class="section">
                        <h3 class="section-title">Project Deliverables</h3>
                        <div class="deliverables-grid">
                            ${project.deliverables.map(item => `<div class="deliverable-item">✅ ${item}</div>`).join('')}
                        </div>
                    </div>
                </div>
            `;
        }
        
        // Generate initial project on page load
        window.onload = function() {
            generateProject();
        };
    </script>
</body>
</html>